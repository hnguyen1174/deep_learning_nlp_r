{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs:\n",
    "\n",
    "* Training Set: 'labeled-data-2019-07-18_14-22.csv': sentence level training dataset.\n",
    "* Test Set: 'gold_standard_HF_150.csv': sentence level test set.\n",
    "* Label: dyspnea.\n",
    "\n",
    "Reason for sentence level:\n",
    "* BERT requires a lot of computational resource and long training time. Training time increases exponentially with the length of the notes. \n",
    "\n",
    "### Outputs:\n",
    "\n",
    "* model_bert_weights_dyspnea_sentences_unbalanced.h5\n",
    "* model_bert_weights_dyspnea_sentences_balanced.h5\n",
    "\n",
    "### References:\n",
    "* https://github.com/strongio/keras-bert/blob/master/keras-bert.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Setting Directories and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "max_seq_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/huynguyen/Desktop/cumc_research/Task1_BERT_or_Elmo\")\n",
    "os.chdir(\"/work/han2114\")\n",
    "df_train_path = os.getcwd() + \"/labeled-data-2019-07-18_14-22.csv\"\n",
    "df_test_path = os.getcwd() + \"/gold_standard_HF_150.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_path_balanced = os.getcwd() + \"/model_bert_weights_dyspnea_sentences_balanced.h5\"\n",
    "bert_model_plot_path_balanced = os.getcwd() + \"/model_bert_weights_dyspnea_sentences_balanced.png\"\n",
    "bert_predict_path_balanced = os.getcwd() + \"/predicts_balanced.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_path_unbalanced = os.getcwd() + \"/model_bert_weights_dyspnea_sentences_unbalanced.h5\"\n",
    "bert_model_plot_path_unbalanced = os.getcwd() + \"/model_bert_weights_dyspnea_sentences_unbalanced.png\"\n",
    "bert_predict_path_unbalanced = os.getcwd() + \"/predicts_unbalanced.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def replace_contraction(text):\n",
    "    contraction_patterns = [(r'won\\'t', 'will not'),\n",
    "                             (r'can\\'t', 'can not'),\n",
    "                             (r'i\\'m', 'i am'),\n",
    "                             (r'ain\\'t', 'is not'),\n",
    "                             (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "                             (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "                             (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "                             (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "                             (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "                             (r'(\\w+)\\'d', '\\g<1> would'),\n",
    "                             (r'&', 'and'),\n",
    "                             (r'dammit', 'damn it'),\n",
    "                             (r'dont', 'do not'),\n",
    "                             (r'wont', 'will not')]\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def replace_links(text, filler=' '):\n",
    "    text = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*',\n",
    "    filler, text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    return text\n",
    "\n",
    "def str_len(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def strip_extra_ws(text):\n",
    "    return text.replace('\\s+', ' ', regex = True)\n",
    "\n",
    "def cleanText(text):\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = replace_contraction(text)\n",
    "    text = replace_links(text, \"link\")\n",
    "    text = remove_numbers(text)\n",
    "    text = re.sub(r'[,!@#$%^&*)(|/><\";:.?\\'\\\\}{]',\"\",text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_training_set(df_train_path, subset = 0.1, balanced = False):\n",
    "    \n",
    "    # Loading and processing the sentence-level dataset\n",
    "    df_train = pd.read_csv(df_train_path)\n",
    "    df_train_dyspnea = df_train[['Note', 'Dyspnea (# of simclins)']]\n",
    "    df_train_dyspnea['Dyspnea (# of simclins)'] = df_train_dyspnea['Dyspnea (# of simclins)'].fillna(0.0)\n",
    "    df_train_dyspnea['dyspnea'] = np.where(df_train_dyspnea['Dyspnea (# of simclins)'] > 0.0, 1, 0)\n",
    "    df_train_dyspnea = df_train_dyspnea[['Note', 'dyspnea']].reset_index()\n",
    "    df_train_dyspnea = df_train_dyspnea.drop('index', axis = 1)\n",
    "    \n",
    "    # Remove rows where 'Note' is empty\n",
    "    df_train_dyspnea = df_train_dyspnea[pd.notnull(df_train_dyspnea['Note'])]\n",
    "    df_train_dyspnea['sent_len'] = df_train_dyspnea['Note'].apply(str_len)\n",
    "    \n",
    "    # Clip the length of 'Note' to 35 words max.\n",
    "    df_train_dyspnea = df_train_dyspnea[df_train_dyspnea['sent_len'] < 35]\n",
    "    \n",
    "    # Subset\n",
    "    df_train_dyspnea = df_train_dyspnea.sample(frac = subset, random_state = 2019)\n",
    "    \n",
    "    if balanced:\n",
    "    # Balance the training set.\n",
    "        df_pos = df_train_dyspnea[df_train_dyspnea['dyspnea'] == 1]\n",
    "        df_neg = df_train_dyspnea[df_train_dyspnea['dyspnea'] == 0].sample(n = df_pos.shape[0], random_state = 2019)\n",
    "        df_train_dyspnea = pd.concat([df_pos, df_neg])\n",
    "        df_train_dyspnea = df_train_dyspnea.reset_index()\n",
    "        df_train_dyspnea.drop('index', inplace = True, axis = 1)\n",
    "    \n",
    "    # Final processing\n",
    "    df_train_dyspnea['Note'] = df_train_dyspnea['Note'].apply(cleanText)\n",
    "    df_train_dyspnea['Note'] = df_train_dyspnea['Note'].str.replace('\\s+', ' ', regex = True)    \n",
    "    \n",
    "    return df_train_dyspnea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = loading_training_set(df_train_path, subset = 0.1)\n",
    "train_balanced = loading_training_set(df_train_path, subset = 1, balanced = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_test_set(df_test_path, balanced = False):\n",
    "    \n",
    "    # Loading and processing the sentence-level dataset\n",
    "    df = pd.read_csv(df_test_path)\n",
    "    for i in range(1, 5):\n",
    "        df['dyspnea_' + str(i)] = np.where(df['Category ' + str(i)] == 'Dyspnea', 1, 0)\n",
    "    df = df[['Note', 'dyspnea_1', 'dyspnea_2', 'dyspnea_3', 'dyspnea_4']]\n",
    "    df['dyspnea'] = df[['dyspnea_1', 'dyspnea_2', 'dyspnea_3', 'dyspnea_4']].sum(axis = 1)\n",
    "    df['dyspnea'] = np.where(df['dyspnea'] > 0, 1, 0)\n",
    "    df = df[['Note', 'dyspnea']]\n",
    "    \n",
    "    # Remove rows where 'Note' is empty\n",
    "    df = df[pd.notnull(df['Note'])]\n",
    "    df['sent_len'] = df['Note'].apply(str_len)\n",
    "    \n",
    "    # Clip the length of 'Note' to 35 words max.\n",
    "    df = df[df['sent_len'] < 35]\n",
    "    if balanced:\n",
    "        df_pos = df[df['dyspnea'] == 1]\n",
    "        df_neg = df[df['dyspnea'] == 0].sample(n = df_pos.shape[0], random_state = 2019)\n",
    "        df = pd.concat([df_pos, df_neg])\n",
    "        df = df.reset_index()\n",
    "        df.drop('index', inplace = True, axis = 1)\n",
    "    \n",
    "    # Final processing\n",
    "    df['Note'] = df['Note'].apply(cleanText)\n",
    "    df['Note'] = df['Note'].str.replace('\\s+', ' ', regex = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = loading_test_set(df_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Preparing Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets (Only take up to max_seq_length words for memory)\n",
    "train_text_bert = train['Note'].tolist()\n",
    "train_text_bert = [' '.join(t.split()[0:max_seq_length]) for t in train_text_bert]\n",
    "train_text_bert = np.array(train_text_bert, dtype=object)[:, np.newaxis]\n",
    "train_label_bert = train['dyspnea'].tolist()\n",
    "\n",
    "test_text_bert = test['Note'].tolist()\n",
    "test_text_bert = [' '.join(t.split()[0:max_seq_length]) for t in test_text_bert]\n",
    "test_text_bert = np.array(test_text_bert, dtype=object)[:, np.newaxis]\n",
    "test_label_bert = test['dyspnea'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets (Only take up to max_seq_length words for memory)\n",
    "train_text_bert_balanced = train_balanced['Note'].tolist()\n",
    "train_text_bert_balanced = [' '.join(t.split()[0:max_seq_length]) for t in train_text_bert_balanced]\n",
    "train_text_bert_balanced = np.array(train_text_bert_balanced, dtype=object)[:, np.newaxis]\n",
    "train_label_bert_balanced = train_balanced['dyspnea'].tolist()\n",
    "test_text_bert = test['Note'].tolist()\n",
    "test_text_bert = [' '.join(t.split()[0:max_seq_length]) for t in test_text_bert]\n",
    "test_text_bert = np.array(test_text_bert, dtype=object)[:, np.newaxis]\n",
    "test_label_bert = test['dyspnea'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=50):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=50):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(train_text_bert, train_label_bert)\n",
    "\n",
    "train_examples = convert_text_to_examples(train_text_bert_balanced, train_label_bert_balanced)\n",
    "test_examples = convert_text_to_examples(test_text_bert, test_label_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",
    ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels\n",
    ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. BERT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "        return pooled\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(max_seq_length): \n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    bert_output = BertLayer(n_fine_tune_layers=3, pooling=\"first\")(bert_inputs)\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(max_seq_length)\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")\n",
    "model.save('model_bert_weights_dyspnea_sentences_balanced.h5')\n",
    "model.save('model_bert_weights_dyspnea_sentences_unbalanced.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear and load model\n",
    "model = None\n",
    "model = build_model(max_seq_length)\n",
    "initialize_vars(sess)\n",
    "model.load_weights('model_bert_weights_dyspnea_sentences_balanced.h5')\n",
    "\n",
    "preds = model.predict([test_input_ids, \n",
    "                       test_input_masks, \n",
    "                       test_segment_ids]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(predicts, df_test):\n",
    "    predicts = np.array(predicts)\n",
    "    predicts = np.where(predicts > 0.5, 1, 0)\n",
    "    y_test = np.array(df_test['dyspnea'])\n",
    "    return confusion_matrix(y_test, predicts), f1_score(y_test, predicts, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(preds, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
